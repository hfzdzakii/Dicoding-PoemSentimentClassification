{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1ce753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hafizh/miniconda3/envs/MainCuda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-12 16:14:06.410469: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-12 16:14:06.423302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744449246.437801  116764 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744449246.442018  116764 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744449246.452843  116764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744449246.452871  116764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744449246.452873  116764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744449246.452874  116764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-12 16:14:06.456742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package words to /home/hafizh/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hafizh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hafizh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hafizh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import unicodedata\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import load_model #type:ignore\n",
    "from tensorflow.keras.utils import pad_sequences # type: ignore\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f037a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_words = set(words.words())\n",
    "\n",
    "def loadCustomDict(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return set(line.strip().lower() for line in file if line.strip())\n",
    "\n",
    "def normalizeWhitespace(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[\\t\\r]+', ' ', text) # Menghapus tab\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text) # Menghilangkan angka\n",
    "    text = re.sub(r'[-â€â€‘â€’â€“â€”â€•]+', '', text)\n",
    "    text = re.sub(r'[_ï¹ï¹Žï¼¿]', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Hilangkan symbol punctuation\n",
    "    text = re.sub(r'\\b(\\w+)(?:\\s+\\1\\b)+', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def removeOtherLanguage(text):\n",
    "    phrase = ' translated'\n",
    "    pos = text.find(phrase)\n",
    "    if pos != -1:\n",
    "        text = text[:pos].rstrip()\n",
    "    text = re.sub(r'\\b\\w*[^\\x00-\\x7F]\\w*\\b', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def removeNonEnglish(text_series, custom_dict):\n",
    "    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in custom_dict) + r')\\b'\n",
    "    temp_series = text_series.str.replace(pattern, '', case=False, regex=True)\n",
    "    split_words = temp_series.str.split()\n",
    "    exploded = split_words.explode()\n",
    "    exploded = exploded[exploded.str.lower().isin(english_words)]\n",
    "    filtered = exploded[~exploded.str.lower().isin(stop_words)]\n",
    "    lemmatized = filtered.apply(lambda word: lemmatizer.lemmatize(word.lower()))\n",
    "    cleaned_text_series = lemmatized.groupby(level=0).agg(' '.join)\n",
    "    pattern2 = r'\\b(\\w+)(?:\\s+\\1\\b)+' #, r'\\1', text)\n",
    "    ser = cleaned_text_series.reindex(text_series.index, fill_value='')\n",
    "    text = ser.str.replace(pattern2, r'\\1', case=False, regex=True)\n",
    "    return text\n",
    "\n",
    "def cleanInference(df):\n",
    "    custom_dict = loadCustomDict('custom_vocab.txt')\n",
    "    df['poem'] = df['poem'].apply(normalizeWhitespace)\n",
    "    df['poem'] = df['poem'].apply(removeOtherLanguage)\n",
    "    df['poem'] = removeNonEnglish(df['poem'], custom_dict)\n",
    "    return df\n",
    "\n",
    "def kerasTokenizer(text, tokenizer):\n",
    "    text_sequence = tokenizer.texts_to_sequences(text)\n",
    "    text_padded = pad_sequences(text_sequence, maxlen=128)\n",
    "    return text_padded\n",
    "\n",
    "def getLabelEncoder(name):\n",
    "    hartmann = ['sadness', 'fear', 'anger', 'joy', 'neutral', 'surprise', 'disgust']\n",
    "    savani = ['joy', 'sadness', 'anger', 'fear', 'love', 'surprise']\n",
    "    deepseek = ['other', 'sadness', 'joy', 'hope', 'love']\n",
    "    if name=='hartmann':\n",
    "        return {i : label for i, label in enumerate(sorted(hartmann))}\n",
    "    if name=='savani':\n",
    "        return {i : label for i, label in enumerate(sorted(savani))}\n",
    "    if name=='deepseek':\n",
    "        return {i : label for i, label in enumerate(sorted(deepseek))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcb03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poem1 = '''\n",
    "Deliverance is not for me in renunciation.\n",
    "I feel the embrace of freedom in a thousand bonds of delight.\n",
    "\n",
    "Thou ever pourest for me the fresh draught of thy wine of various\n",
    "colours and fragrance, filling this earthen vessel to the brim.\n",
    "\n",
    "My world will light its hundred different lamps with thy flame\n",
    "and place them before the altar of thy temple.\n",
    "\n",
    "No, I will never shut the doors of my senses.\n",
    "The delights of sight and hearing and touch will bear thy delight.\n",
    "\n",
    "Yes, all my illusions will burn into illumination of joy,\n",
    "and all my desires ripen into fruits of love.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cedfa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744449252.416426  116764 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "with open(f\"./tokenizer/tokenizer_savani_0.1_lstm.pkl\", \"rb\") as f:\n",
    "    tokenizer_savani = pickle.load(f)\n",
    "with open(f\"./tokenizer/tokenizer_hartmann_0.1_lstm.pkl\", \"rb\") as g:\n",
    "    tokenizer_hartman = pickle.load(g)\n",
    "with open(f\"./tokenizer/tokenizer_deepseek_0.1_lstm.pkl\", \"rb\") as h:\n",
    "    tokenizer_deepseek = pickle.load(h)\n",
    "\n",
    "model_savani = load_model(f\"./model/best_model_savani_0.1_lstm.keras\")\n",
    "model_hartman = load_model(f\"./model/best_model_hartmann_0.1_lstm.keras\")\n",
    "model_deepseek = load_model(f\"./model/best_model_deepseek_0.1_lstm.keras\")\n",
    "\n",
    "MODELS = {\n",
    "    \"savani\": {\n",
    "        \"model\": model_savani,\n",
    "        \"tokenizer\": tokenizer_savani\n",
    "    },\n",
    "    \"hartmann\": {\n",
    "        \"model\": model_hartman,\n",
    "        \"tokenizer\": tokenizer_hartman\n",
    "    },\n",
    "    \"deepseek\": {\n",
    "        \"model\": model_deepseek,\n",
    "        \"tokenizer\": tokenizer_deepseek\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9affe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models = {}\n",
    "\n",
    "def load_model(model_name):\n",
    "    if model_name not in loaded_models:\n",
    "        tokenizer = MODELS[model_name]['tokenizer']\n",
    "        model = MODELS[model_name]['model']\n",
    "        loaded_models[model_name] = (tokenizer, model)\n",
    "    return loaded_models[model_name]\n",
    "    \n",
    "\n",
    "def predict_poem(poem, model_name):\n",
    "    tokenizer, model = load_model(model_name)\n",
    "    poem_df = pd.DataFrame({'poem' : [poem]})\n",
    "    clean_poem_df = cleanInference(poem_df)\n",
    "    text_keras = kerasTokenizer(clean_poem_df['poem'], tokenizer)\n",
    "    result = model.predict(text_keras, verbose=0)\n",
    "    predicted_labels = np.argmax(result, axis=1)\n",
    "    dic = getLabelEncoder(model_name)\n",
    "    return dic[predicted_labels[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b844491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(title=\"NLP Model Text Classifier\") as demo:\n",
    "    gr.Markdown(\"## ðŸ“œ Poem Emotion Classification\")\n",
    "    gr.Markdown(\"\"\"\n",
    "    ### - **Step 1:** Select a labeling technique (model - each has different emotion labels)  \n",
    "    ### - **Step 2:** Enter your poem text  \n",
    "    ### - **Output:** Predicted emotion\n",
    "    \"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model_selector = gr.Dropdown(\n",
    "                choices=list(MODELS.keys()),\n",
    "                value=\"savani\",\n",
    "                interactive=True,\n",
    "                label=\"Select Labelling Technique Model\"\n",
    "            )\n",
    "            text_input = gr.Textbox(\n",
    "                lines=5,\n",
    "                placeholder=\"Enter text here...\",\n",
    "                label=\"Input Text\",\n",
    "                interactive=True\n",
    "            )\n",
    "            submit_btn = gr.Button(\"Classify\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_label = gr.Label(label=\"Classification Results\")\n",
    "            gr.Markdown(\"\"\"\n",
    "            **Poem References**            \n",
    "            - [Poem Hunter](https://www.poemhunter.com)\n",
    "            - [Poem Generator](https://www.poem-generator.org.uk)\n",
    "            - [HelloPoetry](https://hellopoetry.com)\n",
    "            \"\"\")\n",
    "            gr.Markdown(\"\"\"\n",
    "            **Class Available for Each Labelling Model Technique**\n",
    "            - **Hartmann**: ['sadness', 'fear', 'anger', 'joy', 'neutral', 'surprise', 'disgust']\n",
    "            - **Savani**: ['joy', 'sadness', 'anger', 'fear', 'love', 'surprise']\n",
    "            - **Deepseek**: ['other', 'sadness', 'joy', 'hope', 'love']\n",
    "            \"\"\")\n",
    "            \n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=predict_poem,\n",
    "        inputs=[text_input, model_selector],\n",
    "        outputs=[output_label]\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9868e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MainCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
